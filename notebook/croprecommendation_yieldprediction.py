# -*- coding: utf-8 -*-
"""cropRecommendation_yieldPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/tej-mahender/Crop_Recommendation/blob/main/notebook/cropRecommendation_yieldPrediction.ipynb

# **Smart Agriculture: Crop Recommendation & Yield Prediction**

**Abstract:**  This project leverages machine learning to support smart agriculture by recommending the most suitable crop based on environmental features and predicting crop yield from historical agricultural data.

Two ML models have been developed:
1. one for classification (crop recommendation)
2. another for regression (yield prediction)

The system aims to enhance agricultural decision-making for farmers, promoting sustainable farming aligned with the Sustainable Development Goals (SDGs).

#**1. Crop Recommendation Model**
**Overview:**
The Crop Recommendation Model aims to predict the most suitable crop to plant based on environmental and soil parameters such as nutrient content (N, P, K), temperature, humidity, pH, and rainfall. This assists farmers in making informed decisions to maximize yield and sustainability.

**Step 1: Importing Libraries**

**Purpose:** Import necessary Python libraries for data handling, visualization, machine learning, and evaluation.

**Why:** Efficient data manipulation (pandas, numpy), visualization (matplotlib, seaborn), and ML model implementations (sklearn).
"""

# Importing required Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# For Google Colab drive access
from google.colab import drive

# Scikit-learn libraries for model building, preprocessing, evaluation
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Import ML models for classification
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

"""**Step 2: Data Loading and Inspection**
*   Load the dataset from Google Drive.
*  Use .info(), .shape, .head(), .describe() to get an understanding of dataset size, data types, and summary statistics.

**Why:** This step helps identify data quality, data types, missing values, and the general structure before preprocessing.
"""

# Mount Google Drive to access dataset
drive.mount('/content/drive')

# Load dataset
crop = pd.read_csv('/content/drive/MyDrive/Crop_recommendation.csv')

# Basic info about dataset: types, non-null counts, memory usage
crop.info()

# Check dataset shape (rows, columns)
crop.shape

# Display first 5 rows for preview
crop.head()

# Summary statistics of numerical columns
crop.describe()

"""**Step 3: Data Cleaning and Preparation**
*   Rename target variable from '`label`' to '`crop`' for clarity.
*   Check for missing values and duplicates to ensure dataset integrity.

**Why:** Missing values and duplicates can negatively impact model performance and bias results.


"""

# List all column names
crop.columns

# Rename target column 'label' to 'crop' for clarity
crop.rename(columns = {'label':'crop'}, inplace = True)

# Check for missing and duplicate values
print("Missing Values: ", crop.isnull().sum().sum())
print("Duplicate Entries: ", crop.duplicated().sum())

# Display dataset
crop

"""**Step 4: Exploratory Data Analysis (EDA)**

* Analyze unique values and distribution of target labels.
* Visualize data distributions and correlations to understand feature relationships.

**Why:** Understanding data distribution guides feature selection and model choice.
"""

# Checking unique values and data types for each column
for i in crop.columns:
    print("Column Name:", i.upper())
    print("No. of unique values: {}\n".format(crop[i].nunique()))
    # If column is categorical (string), print unique values
    if crop[i].dtype == 'object':
        print('Unique values:', pd.unique(crop[i]))

# Check distribution of target crop labels
crop['crop'].value_counts()

#Visualize Data Distributions and Correlations

# Set plot aesthetics
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 6)

# Visualizing the Distribution of Each Feature
# Plot histograms for all numeric columns to understand their distribution
features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']

for feature in features:
    plt.figure()
    sns.histplot(crop[feature], kde=True, bins=30, color='skyblue')
    plt.title(f'Distribution of {feature.capitalize()}')
    plt.xlabel(feature.capitalize())
    plt.ylabel('Frequency')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Correlation Heatmap
# Calculate the correlation matrix to examine relationships between features
correlation_matrix = crop[features].corr()

# Plot the heatmap with annotations
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix of Features')
plt.show()

"""**Step 5: Label Encoding of Target Variable**

* Encode categorical crop labels into numeric format using LabelEncoder.

**Why:** Machine learning models typically require numerical input for classification targets.
"""

# Encode the categorical target variable into numeric labels for ML algorithms
label_encoder = LabelEncoder()
crop['crop_encoded'] = label_encoder.fit_transform(crop['crop'])
y_encoded = crop['crop_encoded']

# Map label names to their encoded integers for reference
classes = pd.DataFrame({'label': label_encoder.classes_, 'encoded': range(len(label_encoder.classes_))})
classes.set_index('label', inplace=True)
print(classes)

"""**Step 6:** Feature and Target Variable Splitting
* Separate features (N, P, K, temperature, humidity, ph, rainfall) from the target (crop_encoded).

**Why:** ML algorithms require distinct input (features) and output (target) datasets.
"""

# Separate features (X) and target (y)
x = crop.drop(columns=['crop','crop_encoded'])

# Display features
print(x)

#Display target
y_encoded

"""**Step 7: Train-Test Split**
* Split the dataset into training (80%) and testing (20%) subsets.

**Why:** To evaluate model generalization on unseen data and avoid overfitting.
"""

# Split dataset into train and test sets (80% train, 20% test)

x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)

print("x_train :",x_train.shape)
print("x_test :",x_test.shape)
print("y_train :",y_train.shape)
print("y_test :",y_test.shape)

"""**Step 8:** Feature Scaling
* Standardize feature variables to zero mean and unit variance using StandardScaler.

**Why:** Many ML algorithms perform better and converge faster when features are on the same scale.
"""

# Scale features for better model performance (mean=0, std=1)
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test) # Use transform only on test data

"""**Step 9: Model Selection, Training and Model Evaluation**
* Train multiple classification algorithms (Logistic Regression, Naive Bayes, SVM, KNN, Decision Tree, Random Forest).
* Predict on test data and evaluate using accuracy score and confusion matrix.
* Visualize confusion matrices to analyze prediction errors.

**Why:** Comparing different models helps identify the best-performing algorithm for the task.
"""

# Initialize dictionary of classification models to train
models = {
    'Logistic Regression': LogisticRegression(),
    'Naive Bayes': GaussianNB(),
    'Support Vector Machine': SVC(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
}

# Dictionary to store accuracy results for each model
accuracy_results = {}

# Train each model, predict on test set, calculate accuracy, and display confusion matrix
for name, model in models.items():
    model.fit(x_train, y_train)  # Train model
    y_pred = model.predict(x_test)  # Predict on test data
    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy
    accuracy_results[name] = accuracy
    print(f"{name} with accuracy: {accuracy:.4f}")

    # cm = confusion_matrix(y_test, y_pred)

    # Generate confusion matrix plot
    fig, ax = plt.subplots(figsize=(10, 8))  # Set figure size
    ConfusionMatrixDisplay.from_estimator(model, x_test, y_test, ax=ax, cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix - {name}")
    plt.show()
    print("=" * 50)

"""**Step 10: Model Performance Comparison**
* Visualize accuracy scores of all models in a bar chart.

**Why:** Clear comparison allows selection of the best model (Random Forest here).
"""

# Visualize accuracy comparison using bar chart
accuracy_df = pd.DataFrame(list(accuracy_results.items()), columns=['Algorithm', 'Accuracy'])

# Set figure size
plt.figure(figsize=(12, 6))

# Create horizontal bar chart with proper hue setting
sns.barplot(x='Algorithm', y='Accuracy', data=accuracy_df, hue='Algorithm', dodge=False, legend=False)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

# Labels and title
plt.ylabel("Accuracy Score")
plt.xlabel("Machine Learning Algorithm")
plt.title("Comparison of ML Algorithms Based on Accuracy")

# Set accuracy limits
plt.ylim(0, 1)  # Accuracy is between 0 and 1

# Add grid lines for readability
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.show()

"""**Step 11: Final Model Training and Detailed Evaluation**
* Retrain the best model (Random Forest) on the training set.
* Generate classification report (precision, recall, f1-score) and confusion matrix.
* Visualize predicted crop distribution using a pie chart.

**Why:** Confirm the model‚Äôs reliability and interpret the classification results.
"""

# Train final model - Random Forest with optimized parameters
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(x_train, y_train)
y_pred = rf_model.predict(x_test)

# Display final model accuracy and detailed classification report
accuracy = accuracy_score(y_test, y_pred)
print(f"\n‚úÖ Final Model Accuracy (Random Forest): {accuracy:.4f}")
print("\nüìä Classification Report:")
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Plot Confusion Matrix for Final Model
fig, ax = plt.subplots(figsize=(10, 8))
ConfusionMatrixDisplay.from_estimator(rf_model, x_test, y_test, ax=ax, cmap=plt.cm.Blues, display_labels=label_encoder.classes_)
plt.xticks(rotation=45, ha="right")
plt.title("Confusion Matrix - Random Forest")
plt.show()

# Visualize distribution of predicted crops using pie chart

# Predict crops using the trained Random Forest model
y_pred = rf_model.predict(x_test)

# Convert encoded predictions back to crop names
predicted_crops = label_encoder.inverse_transform(y_pred)

# Count occurrences of each predicted crop
crop_counts = pd.Series(predicted_crops).value_counts()

# Plot pie chart
plt.figure(figsize=(10, 8))
plt.pie(crop_counts, labels=crop_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette("viridis", len(crop_counts)))
plt.title("Most Recommended Crops - Model Predictions")
plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular
plt.show()

"""**Step 12: Crop Recommendation Function**
* Interactive function that takes user input for soil and environmental parameters.
* Predicts and recommends the most suitable crop using the trained model.

**Why:** Provides practical application enabling users/farmers to get crop suggestions in real-time.
"""

# Function to recommend crop based on user input features
def recommend_crop():
    print("\nüå± Enter the following values for crop recommendation üå±\n")

    try:
        # Prompt user for input features
        N = float(input("Nitrogen (N): "))
        P = float(input("Phosphorus (P): "))
        K = float(input("Potassium (K): "))
        temperature = float(input("Temperature (¬∞C): "))
        humidity = float(input("Humidity (%): "))
        ph = float(input("pH Level: "))
        rainfall = float(input("Rainfall (mm): "))

        # Prepare input in proper format
        user_input = np.array([[N, P, K, temperature, humidity, ph, rainfall]])

        # Convert user input to DataFrame with correct column names
        user_input_df = pd.DataFrame(user_input, columns=x.columns)

        # Scale user input using previously fitted scaler
        user_input_scaled = scaler.transform(user_input_df)

        # Predict crop label
        predicted_label = rf_model.predict(user_input_scaled)
        crop = label_encoder.inverse_transform(predicted_label)[0]

        print(f"\n‚úÖ Recommended Crop: {crop} üåæ")

    except ValueError:
        print("\n‚ùå Invalid input! Please enter numerical values only.")

# Call the function to test (uncomment below to run in an interactive environment)
recommend_crop()

"""**Step 13:** Saving Crop Recommendation Model
* Save the trained Random Forest model and scaler to disk using pickle.

**Why:** Persist models for future use without retraining, enabling deployment and integration.
"""

# Saving the trained recommendation model using pickle for later use

import pickle

# Reassign for clarity and later saving
crop_recommendation = rf_model

# Save the trained model
with open('/content/crop_recommendation_model.pkl', 'wb') as model_file:
    pickle.dump(crop_recommendation, model_file)

print("‚úÖ Model saved at: /content/crop_recommendation_model.pkl")

# Save scaler as well
with open('scaler_crop_recommendation.pkl', 'wb') as scaler_file:
    pickle.dump(scaler, scaler_file)

# Download the pickle and scaler files
from google.colab import files
files.download('/content/crop_recommendation_model.pkl')
files.download('scaler_crop_recommendation.pkl')

"""**Step 14:** Companion Crop Recommendation via Clustering
* Use KMeans clustering on feature data to identify clusters of compatible crops.
* Recommend companion crops within the same cluster.

**Why:** Companion planting improves crop health and yield by identifying synergistic crop combinations.
"""

data = crop

# Companion Crop Recommendation using Clustering
from sklearn.cluster import KMeans

# Prepare data by removing target label for clustering
companion_data = data.drop(['crop'], axis=1)
crop_labels = data['crop']

# Apply KMeans clustering to group crops based on features
kmeans = KMeans(n_clusters=4, random_state=0)  # Adjust clusters based on dataset
kmeans.fit(companion_data)

# Assign cluster labels back to dataframe
data['cluster'] = kmeans.labels_

# Function to recommend companion crops based on cluster
def recommend_companion(crop_name):
    cluster_id = data[data['crop'] == crop_name]['cluster'].values[0]
    companion_crops = data[data['cluster'] == cluster_id]['crop'].unique()
    companions = [crop for crop in companion_crops if crop != crop_name]
    return companions

# Example Usage
crop_to_check = 'rice'
print(f"Companion crops for {crop_to_check}: {recommend_companion(crop_to_check)}")

#User-facing function to run the prediction system
def run_companion_recommendation():
    print("\033[1müåæ Companion Crop Recommendation System üåæ\033[0m\n")
    crop_input = input("Enter the crop name: ").strip().lower()

    companions = recommend_companion(crop_input)

    if companions:
        print(f"\n\033[1müåø Companion crops for '{crop_input}':\033[0m")
        for crop in companions:
            print(f"  ‚úÖ {crop}")
    else:
        print("\033[1m‚ö†Ô∏è No companion crops found.\033[0m")

# üß™ Example Run
run_companion_recommendation()

# Save trained Companion model
import pickle

# Save the trained KMeans model
with open('/content/companion_crop_kmeans.pkl', 'wb') as model_file:
    pickle.dump(kmeans, model_file)

# Save the fitted scaler
with open('companion_crop_scaler.pkl', 'wb') as scaler_file:
    pickle.dump(scaler, scaler_file)

print("\n‚úÖ Model and Scaler saved successfully!")

# Download the pickle and scaler files
from google.colab import files

files.download('/content/companion_crop_kmeans.pkl')
files.download('companion_crop_scaler.pkl')

"""#**2. Yield Prediction Model**
**Overview:**
The Yield Prediction Model estimates crop production quantity based on historical production data and factors such as year, season, crop type, and cultivated area. This helps in forecasting and planning agricultural outputs.

**Step 1: Importing Libraries**
**Purpose:** Import necessary Python libraries for data handling, visualization, machine learning, and evaluation.

**Why:** Efficient data manipulation (pandas, numpy), visualization (matplotlib, seaborn), and ML model implementations (sklearn).
"""

# Import libraries for data handling, visualization and modeling
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn modules for model building and evaluation
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

"""**Step 2: Data Loading and Cleaning**
* Load crop production dataset.
* Normalize crop names (lowercase, fixing inconsistencies).
* Filter to selected crops relevant for analysis.
* Remove unnecessary columns (State_Name, District_Name).
* Use .info(), .shape, .head(), .describe() to get an understanding of dataset size, data types, and summary statistics.
* Handle missing values by dropping.

**Why:** Ensures clean, consistent data that improves model accuracy.
"""

# Load crop production dataset
crop_data = pd.read_csv("/content/drive/MyDrive/crop_production.csv")

# Normalize crop names by converting to lowercase and fixing inconsistent naming
crop_data['Crop'] = crop_data['Crop'].str.lower()
crop_data['Crop'] = crop_data['Crop'].replace(
    ['moth', 'peas  (vegetable)', 'bean', 'moong(green gram)', 'pome granet',
     'water melon', 'cotton(lint)', 'gram'],
    ['mothbeans', 'pigeonpeas', 'kidneybeans', 'mungbean', 'pomegranate',
     'watermelon', 'cotton', 'chickpea'])

# Select subset of crops relevant for prediction
selected_crops = ['rice', 'maize', 'chickpea', 'kidneybeans', 'pigeonpeas',
                  'mothbeans', 'mungbean', 'blackgram', 'lentil', 'pomegranate',
                  'banana', 'mango', 'grapes', 'watermelon', 'apple',
                  'orange', 'papaya', 'coconut', 'cotton', 'jute', 'coffee']
crop_data = crop_data[crop_data['Crop'].isin(selected_crops)]

# Drop irrelevant columns like state and district names
crop_data = crop_data.drop(['State_Name', 'District_Name'], axis=1)

# Check dataset shape (rows, columns)
crop_data.shape

# Display first 5 rows for preview
crop_data.head()

# Basic info about dataset: types, non-null counts, memory usage
crop_data.info()

# Summary statistics of numerical columns
crop_data.describe()

# List all column names
crop_data.columns

# Check and remove missing values
crop_data.isnull().sum()
crop_data = crop_data.dropna()

# Check for duplicate rows
crop_data.duplicated().sum()

# # Handle missing values
# crop_data = crop_data.dropna()

"""**Step 3: Encoding Categorical Variables**
* Encode categorical features Season and Crop to numeric labels.

**Why:** Most regression algorithms require numeric input for features.
"""

# Encode categorical variables (Season and Crop) to numeric labels
label_encoders = {}
categorical_cols = ['Season', 'Crop']
for col in categorical_cols:
    label_encoders[col] = LabelEncoder()
    crop_data[col] = label_encoders[col].fit_transform(crop_data[col])

"""**Step 4: Feature and Target Definition**
* Define features (Crop_Year, Season, Crop, Area) and target variable (Production).

**Why:** Isolate inputs and outputs for supervised learning.
"""

# Define features and target
X = crop_data.drop(["Production"], axis=1)
y = crop_data["Production"]

#Display features
X

#Display target
y

"""**Step 5: Train-Test Split**
* Split data into 80% training and 20% testing.

**Why:** To validate model performance on unseen data.
"""

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)

print("x_train :",X_train.shape)
print("x_test :",X_test.shape)
print("y_train :",y_train.shape)
print("y_test :",y_test.shape)

"""**Step 6: Feature Scaling**
* Standardize feature variables to zero mean and unit variance using StandardScaler.

**Why:** Many ML algorithms perform better and converge faster when features are on the same scale.
"""

# Standardize feature data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""**Step 7: Model Training and Evaluation**
* Train Random Forest and Decision Tree Regressors.
* Use standardization for Decision Tree inputs.
* Calculate R¬≤, Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE).
* Visualize R¬≤ scores for comparison.

**Why:** Compare two popular regression algorithms to find the better model and Quantify prediction accuracy and error for regression tasks.
"""

# Train Random Forest Regressor
print("Training Random Forest Regressor...")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

# Evaluate Random Forest performance
rf_r2 = r2_score(y_test, rf_pred)
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))

print("\nRandom Forest Results:")
print(f"  R2 Score: {rf_r2:.4f}")
print(f"  Mean Absolute Error: {rf_mae:.2f}")
print(f"  Root Mean Squared Error: {rf_rmse:.2f}")

# # Cross-validation
# rf_cv = cross_val_score(rf_model, X_scaled, y, cv=5, scoring='r2').mean()
# print(f"  Cross-validation R2: {rf_cv:.4f}")
# print("-" * 50)

# Scale data for Decision Tree
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train Decision Tree Regressor
print("Training Decision Tree Regressor...")
dt_model = DecisionTreeRegressor(random_state=5)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)

# Evaluate Decision Tree performance
dt_r2 = r2_score(y_test, dt_pred)
dt_mae = mean_absolute_error(y_test, dt_pred)
dt_rmse = np.sqrt(mean_squared_error(y_test, dt_pred))

print("\nDecision Tree Results:")
print(f"  R2 Score: {dt_r2:.4f}")
print(f"  Mean Absolute Error: {dt_mae:.2f}")
print(f"  Root Mean Squared Error: {dt_rmse:.2f}")

# Visualize and compare model performances using R2 score
models = ["Random Forest", "Decision Tree"]
r2_scores = [rf_r2, dt_r2]


plt.figure(figsize=(10, 5))
sns.barplot(x=models, y=r2_scores, color="skyblue")
plt.ylabel("R2 Score")
plt.title("Model Performance Comparison")
plt.show()

"""**Step 8: Yield Prediction Function**
* Interactive function that accepts user inputs (year, season, crop, area).
* Validates inputs against known categories.
* Predicts and outputs estimated crop production.

**Why:** Allows users to estimate crop yield for planning and management.
"""

# Clean up label encoder classes (remove trailing spaces)
label_encoders['Season'].classes_ = np.array([s.strip() for s in label_encoders['Season'].classes_])
label_encoders['Crop'].classes_ = np.array([c.strip() for c in label_encoders['Crop'].classes_])

# Function for yield prediction based on user input
def predict_yield():
    # Get user input
    crop_year = int(input("Enter Crop Year (e.g., 2024): "))
    season = input("Enter Season (e.g., Kharif, Rabi, Summer, Whole Year): ").strip()
    crop = input("Enter Crop Name (e.g., Rice, Maize, Wheat): ").strip().lower()
    area = float(input("Enter Area in hectares: "))

    # Validate input against known classes
    if season not in label_encoders['Season'].classes_:
        print(f"‚ùå Error: '{season}' is not a valid season!")
        print(f"‚úÖ Available options: {list(label_encoders['Season'].classes_)}")
        return

    if crop not in label_encoders['Crop'].classes_:
        print(f"‚ùå Error: '{crop}' is not a valid crop!")
        print(f"‚úÖ Available options: {list(label_encoders['Crop'].classes_)}")
        return

    # Encode categorical inputa
    season_encoded = label_encoders['Season'].transform([season])[0]
    crop_encoded = label_encoders['Crop'].transform([crop])[0]

    # Prepare feature vector
    input_data = np.array([[crop_year, season_encoded, crop_encoded, area]])

    # Scale features
    input_scaled = scaler.transform(input_data)

    # Predict yield using Random Forest model
    predicted_yield = rf_model.predict(input_scaled)
    print(f"\nüåæ Estimated Crop Production: {predicted_yield[0]:.2f} metric tons")

# Run prediction system
predict_yield()

"""**Step 9: Model Saving**
* Save trained Random Forest model and scaler using pickle.

**Why:** Enables model reuse without retraining, supports deployment.
"""

# Save trained yield prediction model using pickle
import pickle

yield_prediction = rf_model
with open('/content/yield_prediction_model.pkl', 'wb') as model_file:
    pickle.dump(yield_prediction, model_file)

print("‚úÖ Model saved at: /content/yield_prediction_model.pkl")

# Save the scaler as well
with open('yield_prediction_scaler.pkl', 'wb') as scaler_file:
    pickle.dump(scaler, scaler_file)

# Download the pickle and scaler files
from google.colab import files
files.download('/content/yield_prediction_model.pkl')
files.download('yield_prediction_scaler.pkl')